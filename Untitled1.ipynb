{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73035f00-501a-4b3c-99e1-109a1a0f02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from timm.models import create_model\n",
    "from timm.optim import create_optimizer\n",
    "from timm.utils import NativeScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import RandomSampler\n",
    "from pytorch_metric_learning.samplers import MPerClassSampler\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.losses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385e45bb-2d48-4057-841f-f708c57bde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.custom import TuplesDataset\n",
    "from datasets.genericdataset import ImagesFromList\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a655924-b90f-4373-a380-9d7fa257e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deit_small_distilled_patch16_224'\n",
    "drop = 0\n",
    "drop_path= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826cda2-7fb9-407e-9a7c-bf08a0f5ee0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fea6c8a-1687-460e-b4df-bbe53fc87aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "        model,\n",
    "        pretrained=True,\n",
    "        num_classes=0,\n",
    "        drop_rate=drop,\n",
    "        drop_path_rate=drop_path,\n",
    "        drop_block_rate=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc590418-f169-42b5-8cb2-05daa1e6994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TuplesDataset(\n",
    "        data_root=r\"C:\\Users\\scl\\Documents\\AI\\Vision\\Image Retrieval\\data\\Stanford_Online_Products\",\n",
    "        mode='train',\n",
    "        imsize=224,\n",
    "        nnum=5,\n",
    "        qsize=2000,\n",
    "        poolsize=20000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d140cd-b342-42c6-a85c-d43dad54cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Identity()\n",
       "  (head_dist): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293043c0-bb86-4297-88ba-764e1e273c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(\n",
    "        pos_margin=1,\n",
    "        neg_margin=0.7,\n",
    "        distance=CosineSimilarity(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7905657-543a-4406-b56a-1bd8e2330880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_args_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d3a32e-da0a-4aa2-bbe1-50e2acc8ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tuples(batch):\n",
    "    if len(batch) == 1:\n",
    "        return [batch[0][0]], [batch[0][1]]\n",
    "    return [batch[i][0] for i in range(len(batch))], [batch[i][1] for i in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2cab53c-c144-4ba7-874e-48a9de949893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=5, shuffle=True,\n",
    "        num_workers=0, pin_memory=True, sampler=None,\n",
    "        drop_last=True, collate_fn=collate_tuples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14f42c0-dfeb-4489-9175-eab4d282095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> used network: \n",
      ">> Extracting descriptors for query images...\n",
      ">>>> 2000/2000 done...\n",
      ">> Extracting descriptors for negative pool...\n",
      ">>>> 20000/20000 done...\n",
      ">> Searching for hard negatives...\n",
      ">>>> Average negative l2-distance: 33.78\n",
      ">>>> Done\n"
     ]
    }
   ],
   "source": [
    "avg_neg_distance = train_loader.dataset.create_epoch_tuples(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84bd977c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Identity()\n",
       "  (head_dist): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "#model.apply(set_batchnorm_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b969c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd24b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77075,\n",
       " 89413,\n",
       " 86877,\n",
       " 41053,\n",
       " 63381,\n",
       " 97773,\n",
       " 59474,\n",
       " 30227,\n",
       " 37384,\n",
       " 94054,\n",
       " 52117,\n",
       " 48893,\n",
       " 98483,\n",
       " 85000,\n",
       " 56522,\n",
       " 70332,\n",
       " 62969,\n",
       " 76757,\n",
       " 26148,\n",
       " 98978,\n",
       " 23801,\n",
       " 42942,\n",
       " 84731,\n",
       " 103631,\n",
       " 19050,\n",
       " 33861,\n",
       " 73610,\n",
       " 76029,\n",
       " 83888,\n",
       " 61551,\n",
       " 35472,\n",
       " 105149,\n",
       " 79270,\n",
       " 63017,\n",
       " 101287,\n",
       " 20279,\n",
       " 65981,\n",
       " 70127,\n",
       " 4162,\n",
       " 49576,\n",
       " 8619,\n",
       " 100397,\n",
       " 40502,\n",
       " 3261,\n",
       " 34748,\n",
       " 17429,\n",
       " 95650,\n",
       " 58441,\n",
       " 53061,\n",
       " 12517,\n",
       " 33104,\n",
       " 81083,\n",
       " 10531,\n",
       " 10711,\n",
       " 49810,\n",
       " 11630,\n",
       " 72019,\n",
       " 56278,\n",
       " 4197,\n",
       " 13462,\n",
       " 39303,\n",
       " 24610,\n",
       " 87200,\n",
       " 4318,\n",
       " 15911,\n",
       " 85751,\n",
       " 15820,\n",
       " 15300,\n",
       " 99065,\n",
       " 9708,\n",
       " 89587,\n",
       " 78840,\n",
       " 105118,\n",
       " 45617,\n",
       " 50043,\n",
       " 56162,\n",
       " 15668,\n",
       " 19045,\n",
       " 87964,\n",
       " 103893,\n",
       " 78703,\n",
       " 38223,\n",
       " 63962,\n",
       " 22775,\n",
       " 13590,\n",
       " 63834,\n",
       " 82711,\n",
       " 46348,\n",
       " 106680,\n",
       " 72276,\n",
       " 59376,\n",
       " 66339,\n",
       " 32911,\n",
       " 28892,\n",
       " 14066,\n",
       " 24474,\n",
       " 55075,\n",
       " 16232,\n",
       " 72695,\n",
       " 20456,\n",
       " 98392,\n",
       " 73784,\n",
       " 53284,\n",
       " 15072,\n",
       " 57782,\n",
       " 72627,\n",
       " 98255,\n",
       " 71322,\n",
       " 1333,\n",
       " 96928,\n",
       " 92328,\n",
       " 34223,\n",
       " 96252,\n",
       " 78663,\n",
       " 6062,\n",
       " 36343,\n",
       " 33947,\n",
       " 57054,\n",
       " 12789,\n",
       " 31045,\n",
       " 5548,\n",
       " 47360,\n",
       " 104324,\n",
       " 56322,\n",
       " 87504,\n",
       " 36442,\n",
       " 88213,\n",
       " 72779,\n",
       " 103717,\n",
       " 56465,\n",
       " 75071,\n",
       " 62047,\n",
       " 31818,\n",
       " 22142,\n",
       " 10303,\n",
       " 44714,\n",
       " 62372,\n",
       " 24239,\n",
       " 100768,\n",
       " 39572,\n",
       " 40498,\n",
       " 1797,\n",
       " 44520,\n",
       " 73881,\n",
       " 66739,\n",
       " 23508,\n",
       " 53363,\n",
       " 80765,\n",
       " 87844,\n",
       " 291,\n",
       " 70803,\n",
       " 12159,\n",
       " 94614,\n",
       " 8567,\n",
       " 43452,\n",
       " 31115,\n",
       " 39302,\n",
       " 43384,\n",
       " 4036,\n",
       " 99148,\n",
       " 44153,\n",
       " 27825,\n",
       " 106693,\n",
       " 102644,\n",
       " 47573,\n",
       " 23916,\n",
       " 14662,\n",
       " 65762,\n",
       " 78633,\n",
       " 18722,\n",
       " 34872,\n",
       " 20554,\n",
       " 80260,\n",
       " 85222,\n",
       " 90742,\n",
       " 28793,\n",
       " 93125,\n",
       " 1415,\n",
       " 104887,\n",
       " 11770,\n",
       " 65628,\n",
       " 65685,\n",
       " 93320,\n",
       " 55357,\n",
       " 106139,\n",
       " 107402,\n",
       " 85268,\n",
       " 94442,\n",
       " 63055,\n",
       " 37498,\n",
       " 105072,\n",
       " 99726,\n",
       " 79686,\n",
       " 104690,\n",
       " 65212,\n",
       " 106438,\n",
       " 30984,\n",
       " 65407,\n",
       " 7082,\n",
       " 1235,\n",
       " 96029,\n",
       " 85975,\n",
       " 1123,\n",
       " 26944,\n",
       " 51869,\n",
       " 4148,\n",
       " 18932,\n",
       " 103884,\n",
       " 17286,\n",
       " 7880,\n",
       " 66346,\n",
       " 4399,\n",
       " 12400,\n",
       " 14265,\n",
       " 62264,\n",
       " 28283,\n",
       " 45766,\n",
       " 444,\n",
       " 10175,\n",
       " 28942,\n",
       " 975,\n",
       " 85098,\n",
       " 27991,\n",
       " 64646,\n",
       " 58507,\n",
       " 87210,\n",
       " 4312,\n",
       " 4239,\n",
       " 12875,\n",
       " 103203,\n",
       " 23583,\n",
       " 15481,\n",
       " 26273,\n",
       " 20822,\n",
       " 60245,\n",
       " 84278,\n",
       " 21364,\n",
       " 982,\n",
       " 2763,\n",
       " 20295,\n",
       " 59593,\n",
       " 56652,\n",
       " 78413,\n",
       " 104526,\n",
       " 9578,\n",
       " 24757,\n",
       " 78399,\n",
       " 66857,\n",
       " 44069,\n",
       " 53879,\n",
       " 77360,\n",
       " 60167,\n",
       " 30890,\n",
       " 58452,\n",
       " 105850,\n",
       " 3633,\n",
       " 105041,\n",
       " 90002,\n",
       " 101251,\n",
       " 43673,\n",
       " 12193,\n",
       " 75266,\n",
       " 23055,\n",
       " 22863,\n",
       " 87214,\n",
       " 15051,\n",
       " 83507,\n",
       " 20753,\n",
       " 95717,\n",
       " 89718,\n",
       " 8471,\n",
       " 60594,\n",
       " 100141,\n",
       " 12476,\n",
       " 91769,\n",
       " 22112,\n",
       " 51840,\n",
       " 97031,\n",
       " 26334,\n",
       " 70211,\n",
       " 55923,\n",
       " 92172,\n",
       " 52635,\n",
       " 78489,\n",
       " 98306,\n",
       " 28962,\n",
       " 81327,\n",
       " 1343,\n",
       " 13125,\n",
       " 66895,\n",
       " 59652,\n",
       " 104224,\n",
       " 1908,\n",
       " 106340,\n",
       " 103530,\n",
       " 4045,\n",
       " 5179,\n",
       " 83527,\n",
       " 99536,\n",
       " 69860,\n",
       " 75630,\n",
       " 80783,\n",
       " 8637,\n",
       " 40606,\n",
       " 54225,\n",
       " 16575,\n",
       " 17086,\n",
       " 87326,\n",
       " 73200,\n",
       " 82033,\n",
       " 103325,\n",
       " 7002,\n",
       " 23116,\n",
       " 18847,\n",
       " 32518,\n",
       " 60292,\n",
       " 53586,\n",
       " 107574,\n",
       " 2550,\n",
       " 40427,\n",
       " 82083,\n",
       " 41480,\n",
       " 12622,\n",
       " 86986,\n",
       " 25237,\n",
       " 92711,\n",
       " 12033,\n",
       " 19403,\n",
       " 51226,\n",
       " 6222,\n",
       " 80698,\n",
       " 68151,\n",
       " 4043,\n",
       " 98847,\n",
       " 79748,\n",
       " 92194,\n",
       " 59818,\n",
       " 49784,\n",
       " 27531,\n",
       " 98246,\n",
       " 44982,\n",
       " 64848,\n",
       " 56619,\n",
       " 55163,\n",
       " 68406,\n",
       " 82420,\n",
       " 16272,\n",
       " 61622,\n",
       " 53087,\n",
       " 68463,\n",
       " 101123,\n",
       " 17739,\n",
       " 32346,\n",
       " 99175,\n",
       " 45881,\n",
       " 35176,\n",
       " 38207,\n",
       " 47905,\n",
       " 43994,\n",
       " 45750,\n",
       " 51786,\n",
       " 23788,\n",
       " 78123,\n",
       " 5737,\n",
       " 86742,\n",
       " 45932,\n",
       " 45742,\n",
       " 57331,\n",
       " 76293,\n",
       " 29243,\n",
       " 107009,\n",
       " 76236,\n",
       " 58129,\n",
       " 40179,\n",
       " 16774,\n",
       " 8075,\n",
       " 34613,\n",
       " 95727,\n",
       " 23620,\n",
       " 25646,\n",
       " 65916,\n",
       " 89073,\n",
       " 16070,\n",
       " 329,\n",
       " 97704,\n",
       " 88229,\n",
       " 8410,\n",
       " 12494,\n",
       " 86410,\n",
       " 6350,\n",
       " 55473,\n",
       " 14300,\n",
       " 45025,\n",
       " 86850,\n",
       " 42223,\n",
       " 54505,\n",
       " 19708,\n",
       " 62282,\n",
       " 46807,\n",
       " 44267,\n",
       " 67329,\n",
       " 20536,\n",
       " 54081,\n",
       " 16592,\n",
       " 43435,\n",
       " 2071,\n",
       " 40814,\n",
       " 54144,\n",
       " 32233,\n",
       " 28029,\n",
       " 16186,\n",
       " 105092,\n",
       " 10213,\n",
       " 13201,\n",
       " 54860,\n",
       " 955,\n",
       " 69761,\n",
       " 82171,\n",
       " 31589,\n",
       " 11033,\n",
       " 101095,\n",
       " 70790,\n",
       " 53969,\n",
       " 78164,\n",
       " 15829,\n",
       " 27618,\n",
       " 74818,\n",
       " 50859,\n",
       " 16443,\n",
       " 38869,\n",
       " 80719,\n",
       " 13186,\n",
       " 39054,\n",
       " 46798,\n",
       " 80030,\n",
       " 63236,\n",
       " 94798,\n",
       " 27296,\n",
       " 7751,\n",
       " 77335,\n",
       " 25312,\n",
       " 79610,\n",
       " 2828,\n",
       " 68433,\n",
       " 5973,\n",
       " 101129,\n",
       " 1428,\n",
       " 62222,\n",
       " 80549,\n",
       " 3370,\n",
       " 9164,\n",
       " 30699,\n",
       " 33962,\n",
       " 85080,\n",
       " 8019,\n",
       " 82095,\n",
       " 65638,\n",
       " 101644,\n",
       " 11156,\n",
       " 52270,\n",
       " 79756,\n",
       " 14348,\n",
       " 91419,\n",
       " 73345,\n",
       " 83582,\n",
       " 73853,\n",
       " 11732,\n",
       " 1523,\n",
       " 107721,\n",
       " 92628,\n",
       " 18838,\n",
       " 8502,\n",
       " 16020,\n",
       " 24399,\n",
       " 30879,\n",
       " 14938,\n",
       " 106571,\n",
       " 62802,\n",
       " 42333,\n",
       " 19007,\n",
       " 32698,\n",
       " 45965,\n",
       " 2327,\n",
       " 4275,\n",
       " 99335,\n",
       " 85705,\n",
       " 79213,\n",
       " 58528,\n",
       " 48938,\n",
       " 10374,\n",
       " 38977,\n",
       " 54292,\n",
       " 9150,\n",
       " 107365,\n",
       " 84634,\n",
       " 102847,\n",
       " 2213,\n",
       " 30036,\n",
       " 37424,\n",
       " 80872,\n",
       " 102650,\n",
       " 8289,\n",
       " 23561,\n",
       " 97588,\n",
       " 104109,\n",
       " 105994,\n",
       " 30633,\n",
       " 97596,\n",
       " 15210,\n",
       " 5874,\n",
       " 3400,\n",
       " 9255,\n",
       " 44718,\n",
       " 90532,\n",
       " 71904,\n",
       " 88791,\n",
       " 94691,\n",
       " 26109,\n",
       " 105919,\n",
       " 84631,\n",
       " 53309,\n",
       " 102130,\n",
       " 349,\n",
       " 65202,\n",
       " 9988,\n",
       " 28438,\n",
       " 54717,\n",
       " 68351,\n",
       " 90372,\n",
       " 31756,\n",
       " 7269,\n",
       " 57023,\n",
       " 46291,\n",
       " 25057,\n",
       " 68728,\n",
       " 4692,\n",
       " 103532,\n",
       " 12863,\n",
       " 107,\n",
       " 31083,\n",
       " 47086,\n",
       " 56108,\n",
       " 100384,\n",
       " 55238,\n",
       " 13221,\n",
       " 9752,\n",
       " 40215,\n",
       " 99617,\n",
       " 31375,\n",
       " 85002,\n",
       " 107875,\n",
       " 33779,\n",
       " 50188,\n",
       " 48650,\n",
       " 8407,\n",
       " 26685,\n",
       " 64467,\n",
       " 19947,\n",
       " 6424,\n",
       " 35118,\n",
       " 92584,\n",
       " 19431,\n",
       " 67553,\n",
       " 28394,\n",
       " 18489,\n",
       " 91003,\n",
       " 51067,\n",
       " 7492,\n",
       " 106138,\n",
       " 54760,\n",
       " 106852,\n",
       " 88853,\n",
       " 52411,\n",
       " 85702,\n",
       " 73559,\n",
       " 97040,\n",
       " 96404,\n",
       " 97003,\n",
       " 96769,\n",
       " 102852,\n",
       " 85236,\n",
       " 23662,\n",
       " 56331,\n",
       " 22896,\n",
       " 12624,\n",
       " 9975,\n",
       " 77893,\n",
       " 68382,\n",
       " 38266,\n",
       " 99962,\n",
       " 43991,\n",
       " 8339,\n",
       " 3254,\n",
       " 49010,\n",
       " 92652,\n",
       " 55654,\n",
       " 6973,\n",
       " 86385,\n",
       " 79156,\n",
       " 911,\n",
       " 64375,\n",
       " 29381,\n",
       " 99225,\n",
       " 64361,\n",
       " 67785,\n",
       " 101985,\n",
       " 107118,\n",
       " 41270,\n",
       " 55229,\n",
       " 74527,\n",
       " 104960,\n",
       " 20741,\n",
       " 38391,\n",
       " 100305,\n",
       " 658,\n",
       " 78918,\n",
       " 4165,\n",
       " 3905,\n",
       " 80141,\n",
       " 87365,\n",
       " 89789,\n",
       " 101371,\n",
       " 6964,\n",
       " 79984,\n",
       " 13876,\n",
       " 16428,\n",
       " 86218,\n",
       " 62994,\n",
       " 49090,\n",
       " 7777,\n",
       " 36632,\n",
       " 81428,\n",
       " 52047,\n",
       " 107681,\n",
       " 101834,\n",
       " 79185,\n",
       " 24893,\n",
       " 68021,\n",
       " 1375,\n",
       " 69770,\n",
       " 64199,\n",
       " 10235,\n",
       " 87241,\n",
       " 100433,\n",
       " 995,\n",
       " 57838,\n",
       " 2426,\n",
       " 52582,\n",
       " 37570,\n",
       " 23490,\n",
       " 45175,\n",
       " 39634,\n",
       " 13928,\n",
       " 7957,\n",
       " 17531,\n",
       " 32758,\n",
       " 26786,\n",
       " 48669,\n",
       " 105613,\n",
       " 59900,\n",
       " 63015,\n",
       " 91014,\n",
       " 2996,\n",
       " 7092,\n",
       " 25141,\n",
       " 23309,\n",
       " 55597,\n",
       " 68661,\n",
       " 51690,\n",
       " 24931,\n",
       " 77750,\n",
       " 43521,\n",
       " 25137,\n",
       " 8831,\n",
       " 22534,\n",
       " 89601,\n",
       " 49637,\n",
       " 87656,\n",
       " 15359,\n",
       " 69816,\n",
       " 42990,\n",
       " 60974,\n",
       " 22260,\n",
       " 104076,\n",
       " 53832,\n",
       " 6826,\n",
       " 16415,\n",
       " 104240,\n",
       " 54277,\n",
       " 48903,\n",
       " 32801,\n",
       " 3974,\n",
       " 20886,\n",
       " 100401,\n",
       " 61909,\n",
       " 10869,\n",
       " 64297,\n",
       " 40380,\n",
       " 58015,\n",
       " 75787,\n",
       " 70891,\n",
       " 75621,\n",
       " 70086,\n",
       " 41902,\n",
       " 53358,\n",
       " 69191,\n",
       " 37174,\n",
       " 88006,\n",
       " 39669,\n",
       " 74885,\n",
       " 85392,\n",
       " 103072,\n",
       " 73193,\n",
       " 72168,\n",
       " 45653,\n",
       " 10099,\n",
       " 61928,\n",
       " 52789,\n",
       " 27265,\n",
       " 107492,\n",
       " 62964,\n",
       " 83158,\n",
       " 83681,\n",
       " 47942,\n",
       " 6265,\n",
       " 3587,\n",
       " 107541,\n",
       " 53493,\n",
       " 22317,\n",
       " 21309,\n",
       " 74287,\n",
       " 85177,\n",
       " 26618,\n",
       " 63072,\n",
       " 82291,\n",
       " 99820,\n",
       " 102584,\n",
       " 45350,\n",
       " 104269,\n",
       " 47755,\n",
       " 10429,\n",
       " 5707,\n",
       " 57699,\n",
       " 99366,\n",
       " 71113,\n",
       " 2078,\n",
       " 50760,\n",
       " 74528,\n",
       " 56505,\n",
       " 52101,\n",
       " 15492,\n",
       " 19883,\n",
       " 2714,\n",
       " 3494,\n",
       " 84091,\n",
       " 25811,\n",
       " 55639,\n",
       " 64789,\n",
       " 107020,\n",
       " 12126,\n",
       " 26091,\n",
       " 7546,\n",
       " 26311,\n",
       " 103922,\n",
       " 65967,\n",
       " 48718,\n",
       " 10112,\n",
       " 87497,\n",
       " 65175,\n",
       " 102144,\n",
       " 3818,\n",
       " 72242,\n",
       " 32648,\n",
       " 51873,\n",
       " 97659,\n",
       " 77007,\n",
       " 87991,\n",
       " 31153,\n",
       " 103959,\n",
       " 34433,\n",
       " 25574,\n",
       " 64052,\n",
       " 1455,\n",
       " 48246,\n",
       " 27928,\n",
       " 107831,\n",
       " 61596,\n",
       " 58357,\n",
       " 76580,\n",
       " 78055,\n",
       " 98815,\n",
       " 58978,\n",
       " 16930,\n",
       " 93644,\n",
       " 37523,\n",
       " 79987,\n",
       " 107581,\n",
       " 56798,\n",
       " 31498,\n",
       " 15548,\n",
       " 102448,\n",
       " 29972,\n",
       " 41836,\n",
       " 103649,\n",
       " 34962,\n",
       " 51789,\n",
       " 8041,\n",
       " 70785,\n",
       " 15710,\n",
       " 20578,\n",
       " 86568,\n",
       " 79215,\n",
       " 82400,\n",
       " 2547,\n",
       " 98955,\n",
       " 71456,\n",
       " 72165,\n",
       " 2641,\n",
       " 31825,\n",
       " 20965,\n",
       " 106120,\n",
       " 103125,\n",
       " 104753,\n",
       " 72528,\n",
       " 87278,\n",
       " 16073,\n",
       " 12114,\n",
       " 5554,\n",
       " 21070,\n",
       " 49328,\n",
       " 87836,\n",
       " 33815,\n",
       " 100695,\n",
       " 85100,\n",
       " 54424,\n",
       " 44967,\n",
       " 66333,\n",
       " 79823,\n",
       " 23388,\n",
       " 27755,\n",
       " 11648,\n",
       " 7634,\n",
       " 44771,\n",
       " 4713,\n",
       " 3800,\n",
       " 79722,\n",
       " 77316,\n",
       " 92477,\n",
       " 62234,\n",
       " 21225,\n",
       " 38143,\n",
       " 22153,\n",
       " 71614,\n",
       " 25455,\n",
       " 55792,\n",
       " 13831,\n",
       " 62117,\n",
       " 43864,\n",
       " 43350,\n",
       " 6718,\n",
       " 64739,\n",
       " 103492,\n",
       " 42669,\n",
       " 90136,\n",
       " 10946,\n",
       " 60121,\n",
       " 37079,\n",
       " 103411,\n",
       " 72542,\n",
       " 82790,\n",
       " 52447,\n",
       " 62689,\n",
       " 16035,\n",
       " 32448,\n",
       " 24570,\n",
       " 67672,\n",
       " 78957,\n",
       " 64667,\n",
       " 98143,\n",
       " 54581,\n",
       " 44433,\n",
       " 64155,\n",
       " 65124,\n",
       " 103526,\n",
       " 57973,\n",
       " 72454,\n",
       " 76480,\n",
       " 95755,\n",
       " 77617,\n",
       " 35011,\n",
       " 51930,\n",
       " 19161,\n",
       " 31629,\n",
       " 6824,\n",
       " 63922,\n",
       " 49432,\n",
       " 46867,\n",
       " 105753,\n",
       " 33113,\n",
       " 51699,\n",
       " 26492,\n",
       " 65713,\n",
       " 65128,\n",
       " 39766,\n",
       " 14307,\n",
       " 50736,\n",
       " 47436,\n",
       " 20679,\n",
       " 74168,\n",
       " 14192,\n",
       " 77677,\n",
       " 107605,\n",
       " 101235,\n",
       " 68389,\n",
       " 32630,\n",
       " 36372,\n",
       " 29432,\n",
       " 95165,\n",
       " 38472,\n",
       " 22577,\n",
       " 93982,\n",
       " 33609,\n",
       " 81866,\n",
       " 66756,\n",
       " 11560,\n",
       " 91396,\n",
       " 78737,\n",
       " 43653,\n",
       " 51426,\n",
       " 100917,\n",
       " 57228,\n",
       " 99158,\n",
       " 24524,\n",
       " 97531,\n",
       " 31071,\n",
       " 86006,\n",
       " 38521,\n",
       " 369,\n",
       " 80058,\n",
       " 56829,\n",
       " 88087,\n",
       " 24920,\n",
       " 10806,\n",
       " 24015,\n",
       " 68673,\n",
       " 93010,\n",
       " 47301,\n",
       " 8664,\n",
       " 67243,\n",
       " 36841,\n",
       " 17933,\n",
       " 3927,\n",
       " 8972,\n",
       " 69305,\n",
       " 101499,\n",
       " 70624,\n",
       " 59614,\n",
       " 66512,\n",
       " 14774,\n",
       " 13028,\n",
       " 490,\n",
       " 87111,\n",
       " 65292,\n",
       " 91809,\n",
       " 96872,\n",
       " 37825,\n",
       " 55038,\n",
       " 43364,\n",
       " 10545,\n",
       " 67690,\n",
       " 55831,\n",
       " 60578,\n",
       " 7662,\n",
       " 4105,\n",
       " 12499,\n",
       " 22820,\n",
       " 10475,\n",
       " 6863,\n",
       " 34856,\n",
       " 79489,\n",
       " 44800,\n",
       " 41286,\n",
       " 41657,\n",
       " 44121,\n",
       " 67804,\n",
       " 79233,\n",
       " 4684,\n",
       " 51984,\n",
       " 23093,\n",
       " 69048,\n",
       " 11964,\n",
       " 20610,\n",
       " 12425,\n",
       " 45474,\n",
       " 81362,\n",
       " 77619,\n",
       " 6515,\n",
       " 28112,\n",
       " 65989,\n",
       " 3593,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.qidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "884f8c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (827628693.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    num workers=0\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num workers=0\n",
    "for i in train_loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc3440e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# measure data loading time\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#data_time.update(time.time() - end)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     nq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;66;03m# number of training tuples\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     ni \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# number of images per tuple\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\retrieval\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\retrieval\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\retrieval\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\retrieval\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Documents\\AI\\Vision\\Image Retrieval\\image-retrieval-transformers\\datasets\\custom.py:113\u001b[0m, in \u001b[0;36mTuplesDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    111\u001b[0m output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqidxs[index]]))\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# positive image\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpidxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# negative images\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnidxs[index])):\n",
      "File \u001b[1;32m~\\Documents\\AI\\Vision\\Image Retrieval\\image-retrieval-transformers\\datasets\\datahelpers.py:23\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     24\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(train_loader):\n",
    "    # measure data loading time\n",
    "    #data_time.update(time.time() - end)\n",
    "\n",
    "\n",
    "    nq = len(input) # number of training tuples\n",
    "    ni = len(input[0]) # number of images per tuple\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd188568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49787869",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq = len(input) # number of training tuples\n",
    "ni = len(input[0]) # number of images per tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a3548ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni,nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1521c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def contrastive_loss(x, label, margin=0.7, eps=1e-6):\n",
    "    # x is D x N\n",
    "    dim = x.size(0) # D\n",
    "    nq = torch.sum(label.data==-1) # number of tuples\n",
    "    S = x.size(1) // nq # number of images per tuple including query: 1+1+n\n",
    "\n",
    "    x1 = x[:, ::S].permute(1,0).repeat(1,S-1).view((S-1)*nq,dim).permute(1,0)\n",
    "    idx = [i for i in range(len(label)) if label.data[i] != -1]\n",
    "    x2 = x[:, idx]\n",
    "    lbl = label[label!=-1]\n",
    "\n",
    "    dif = x1 - x2\n",
    "    D = torch.pow(dif+eps, 2).sum(dim=0).sqrt()\n",
    "\n",
    "    y = 0.5*lbl*torch.pow(D,2) + 0.5*(1-lbl)*torch.pow(torch.clamp(margin-D, min=0),2)\n",
    "    y = torch.sum(y)\n",
    "    return y\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    r\"\"\"CONTRASTIVELOSS layer that computes contrastive loss for a batch of images:\n",
    "        Q query tuples, each packed in the form of (q,p,n1,..nN)\n",
    "\n",
    "    Args:\n",
    "        x: tuples arranges in columns as [q,p,n1,nN, ... ]\n",
    "        label: -1 for query, 1 for corresponding positive, 0 for corresponding negative\n",
    "        margin: contrastive loss margin. Default: 0.7\n",
    "\n",
    "    >>> contrastive_loss = ContrastiveLoss(margin=0.7)\n",
    "    >>> input = torch.randn(128, 35, requires_grad=True)\n",
    "    >>> label = torch.Tensor([-1, 1, 0, 0, 0, 0, 0] * 5)\n",
    "    >>> output = contrastive_loss(input, label)\n",
    "    >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.7, eps=1e-6):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        return contrastive_loss(x, label, margin=self.margin, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'margin=' + '{:.4f}'.format(self.margin) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20bda1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=.7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3565d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(nq):\n",
    "    output = torch.zeros(384, ni).cuda()\n",
    "    for imi in range(ni):\n",
    "        # compute output vector for image imi\n",
    "        output[:, imi] = model(input[q][imi].cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "658ef474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[q].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9411621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, tensor(1, device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = output.size(0) # D\n",
    "nq = torch.sum(target[q].cuda().data==-1) # number of tuples\n",
    "dim,nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f6483d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7, device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = output.size(1) // nq # number of images per tuple including query: 1+1+n\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa8fb24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 6])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = output[:, ::S].permute(1,0).repeat(1,S-1).view((S-1)*nq,dim).permute(1,0)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07cf882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 6])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i in range(len(target[q].cuda())) if target[q].cuda().data[i] != -1]\n",
    "idx\n",
    "x2 = output[:, idx]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e05ffd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl = target[q].cuda()[target[q].cuda()!=-1]\n",
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e983d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = x1 - x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5b54bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8516, -5.0933, -4.1675, -2.2044, -0.2931, -3.0903],\n",
       "        [ 0.3798,  1.6376,  2.5188,  1.7745,  0.9183,  0.4143],\n",
       "        [-1.3690,  0.1948, -1.0262, -0.6256, -1.9431, -4.4646],\n",
       "        ...,\n",
       "        [-0.3437, -0.1261, -1.9117, -0.8146,  1.4371,  1.4417],\n",
       "        [-2.9694,  0.6450, -1.7402, -1.3156, -1.8560, -0.7339],\n",
       "        [ 1.3170, -0.9119,  1.0309,  3.2296,  1.8558,  3.9030]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a45b55b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(dif\u001b[38;5;241m+\u001b[39m\u001b[43meps\u001b[49m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eps' is not defined"
     ]
    }
   ],
   "source": [
    "D = torch.pow(dif+eps, 2).sum(dim=0).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e48ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = x.size(1) // nq # number of images per tuple including query: 1+1+n\n",
    "\n",
    "x1 = x[:, ::S].permute(1,0).repeat(1,S-1).view((S-1)*nq,dim).permute(1,0)\n",
    "idx = [i for i in range(len(label)) if label.data[i] != -1]\n",
    "x2 = x[:, idx]\n",
    "lbl = label[label!=-1]\n",
    "\n",
    "dif = x1 - x2\n",
    "D = torch.pow(dif+eps, 2).sum(dim=0).sqrt()\n",
    "\n",
    "y = 0.5*lbl*torch.pow(D,2) + 0.5*(1-lbl)*torch.pow(torch.clamp(margin-D, min=0),2)\n",
    "y = torch.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fb7606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.3481,  0.3652,  0.3309,  ..., -1.2788, -1.2788, -1.2959],\n",
       "           [ 0.3994,  0.3823,  0.3481,  ..., -1.0219, -1.0904, -1.1589],\n",
       "           [ 0.4166,  0.3481,  0.3481,  ..., -0.8678, -0.9020, -0.9705],\n",
       "           ...,\n",
       "           [ 0.3481,  0.1768,  0.1939,  ...,  0.9988,  1.0331,  0.9817],\n",
       "           [ 0.7419,  0.7248,  0.6392,  ...,  0.9988,  1.0673,  1.0159],\n",
       "           [ 0.5022,  0.5364,  0.4508,  ...,  1.0502,  1.0502,  0.9988]],\n",
       " \n",
       "          [[-0.0574, -0.0049, -0.0224,  ..., -1.3179, -1.3179, -1.3529],\n",
       "           [ 0.0126, -0.0049, -0.0049,  ..., -1.0903, -1.1779, -1.2479],\n",
       "           [ 0.0301, -0.0049,  0.0126,  ..., -0.9153, -0.9853, -1.0203],\n",
       "           ...,\n",
       "           [ 0.5553,  0.3803,  0.3978,  ...,  1.0630,  1.0980,  1.0455],\n",
       "           [ 0.8704,  0.8354,  0.7479,  ...,  1.0630,  1.1331,  1.0805],\n",
       "           [ 0.4853,  0.5378,  0.4153,  ...,  1.1155,  1.1155,  1.0630]],\n",
       " \n",
       "          [[-0.4798, -0.4450, -0.4798,  ..., -1.4384, -1.4210, -1.4384],\n",
       "           [-0.4275, -0.4450, -0.4624,  ..., -1.2467, -1.3164, -1.3687],\n",
       "           [-0.3753, -0.4450, -0.4450,  ..., -1.1770, -1.1944, -1.1770],\n",
       "           ...,\n",
       "           [ 0.4091,  0.2173,  0.1999,  ...,  0.7751,  0.7925,  0.7576],\n",
       "           [ 0.7228,  0.6705,  0.5659,  ...,  0.7751,  0.8448,  0.7925],\n",
       "           [ 0.3045,  0.3742,  0.3219,  ...,  0.8274,  0.8274,  0.7751]]]]),\n",
       " tensor([[[[1.3242, 1.3242, 1.2728,  ..., 2.2318, 2.1975, 2.1462],\n",
       "           [1.3413, 1.2728, 1.2385,  ..., 2.2318, 2.2318, 2.1975],\n",
       "           [1.4098, 1.3242, 1.3242,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [1.4098, 1.4269, 1.3755,  ..., 0.6563, 0.6734, 0.7248],\n",
       "           [1.4612, 1.4612, 1.4098,  ..., 0.6906, 0.7248, 0.7248],\n",
       "           [1.4783, 1.4612, 1.4269,  ..., 0.6734, 0.7077, 0.6734]],\n",
       " \n",
       "          [[1.3256, 1.3431, 1.2906,  ..., 2.4286, 2.4286, 2.4111],\n",
       "           [1.3431, 1.2906, 1.2556,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [1.4132, 1.3431, 1.3431,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [1.6057, 1.6232, 1.5707,  ..., 0.6779, 0.6954, 0.7304],\n",
       "           [1.6583, 1.6583, 1.6057,  ..., 0.7129, 0.7129, 0.7129],\n",
       "           [1.6758, 1.6583, 1.6232,  ..., 0.6954, 0.6954, 0.6604]],\n",
       " \n",
       "          [[1.2457, 1.2108, 1.1759,  ..., 2.6051, 2.6226, 2.5877],\n",
       "           [1.2631, 1.1759, 1.1411,  ..., 2.6400, 2.6400, 2.6226],\n",
       "           [1.3328, 1.2282, 1.2282,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [1.6291, 1.6465, 1.5942,  ..., 0.5659, 0.5834, 0.6182],\n",
       "           [1.6814, 1.6814, 1.6291,  ..., 0.6182, 0.6356, 0.6356],\n",
       "           [1.6988, 1.6814, 1.6465,  ..., 0.6008, 0.6182, 0.5834]]]]),\n",
       " tensor([[[[ 1.9064,  1.9064,  1.9064,  ...,  1.9578,  1.9578,  1.9407],\n",
       "           [ 1.8379,  1.8379,  1.8379,  ...,  1.9578,  1.9407,  1.9235],\n",
       "           [ 1.8550,  1.8550,  1.8550,  ...,  1.9578,  1.9407,  1.9064],\n",
       "           ...,\n",
       "           [-2.0152, -2.0837, -2.1179,  ...,  0.7591,  0.7762,  0.7762],\n",
       "           [-2.0152, -2.1179, -2.1179,  ...,  0.8276,  0.8618,  0.8961],\n",
       "           [-2.1008, -2.1179, -2.1008,  ...,  0.8618,  0.8789,  0.9132]],\n",
       " \n",
       "          [[ 1.7808,  1.7808,  1.7808,  ...,  1.7633,  1.7983,  1.7983],\n",
       "           [ 1.7283,  1.7283,  1.7283,  ...,  1.7633,  1.7808,  1.7808],\n",
       "           [ 1.7458,  1.7458,  1.7458,  ...,  1.7633,  1.7808,  1.7633],\n",
       "           ...,\n",
       "           [-1.2654, -1.4055, -1.5280,  ...,  1.1155,  1.0980,  1.0805],\n",
       "           [-1.1429, -1.4230, -1.4230,  ...,  1.1506,  1.1681,  1.1856],\n",
       "           [-1.1253, -1.2654, -1.2654,  ...,  1.1506,  1.1681,  1.2031]],\n",
       " \n",
       "          [[ 1.1585,  1.1411,  1.1411,  ...,  1.1585,  1.1759,  1.1759],\n",
       "           [ 1.0888,  1.0888,  1.0888,  ...,  1.1585,  1.1759,  1.1585],\n",
       "           [ 1.1062,  1.1062,  1.1062,  ...,  1.2108,  1.1934,  1.1759],\n",
       "           ...,\n",
       "           [-0.6367, -0.6367, -0.6193,  ...,  0.9842,  0.9668,  0.9319],\n",
       "           [-0.5321, -0.8110, -0.6890,  ...,  1.0017,  1.0365,  1.0365],\n",
       "           [-0.4624, -0.6367, -0.6018,  ...,  1.0017,  1.0191,  1.0365]]]]),\n",
       " tensor([[[[-1.6555, -1.6727, -1.6384,  ..., -1.6213, -1.6213, -1.6042],\n",
       "           [-1.6555, -1.6898, -1.6727,  ..., -1.6042, -1.5870, -1.5699],\n",
       "           [-1.6555, -1.6384, -1.6384,  ..., -1.6042, -1.5870, -1.5870],\n",
       "           ...,\n",
       "           [ 0.2453,  0.2282,  0.2453,  ..., -0.0458, -0.0629, -0.0801],\n",
       "           [ 0.1426,  0.1426,  0.0912,  ..., -0.0458, -0.0458, -0.0458],\n",
       "           [ 0.0056, -0.0287, -0.0972,  ..., -0.0287, -0.0287, -0.0287]],\n",
       " \n",
       "          [[-1.6155, -1.6681, -1.6331,  ..., -1.5805, -1.5805, -1.5630],\n",
       "           [-1.6155, -1.6856, -1.6681,  ..., -1.5805, -1.5630, -1.5455],\n",
       "           [-1.5980, -1.6331, -1.6331,  ..., -1.5805, -1.5630, -1.5630],\n",
       "           ...,\n",
       "           [ 0.2927,  0.2752,  0.2927,  ...,  0.1527,  0.1352,  0.1176],\n",
       "           [ 0.1877,  0.1877,  0.1352,  ...,  0.1527,  0.1527,  0.1527],\n",
       "           [ 0.0476,  0.0126, -0.0574,  ...,  0.1702,  0.1702,  0.1702]],\n",
       " \n",
       "          [[-1.3164, -1.3339, -1.2990,  ..., -1.3339, -1.3339, -1.3164],\n",
       "           [-1.3164, -1.3513, -1.3339,  ..., -1.3339, -1.3164, -1.2990],\n",
       "           [-1.3164, -1.2990, -1.2990,  ..., -1.3339, -1.3164, -1.3164],\n",
       "           ...,\n",
       "           [ 0.6182,  0.6008,  0.6182,  ...,  0.5834,  0.5659,  0.5485],\n",
       "           [ 0.5136,  0.5136,  0.4614,  ...,  0.5834,  0.5834,  0.5834],\n",
       "           [ 0.3742,  0.3393,  0.2696,  ...,  0.6008,  0.6008,  0.6008]]]]),\n",
       " tensor([[[[-0.9534, -0.8507, -0.8335,  ..., -1.4500, -1.4158, -1.3815],\n",
       "           [-0.9020, -0.9363, -0.9363,  ..., -1.3473, -1.4158, -1.4500],\n",
       "           [-0.8507, -0.9020, -0.9020,  ..., -1.3473, -1.4158, -1.3815],\n",
       "           ...,\n",
       "           [-1.0048, -0.9363, -0.9020,  ..., -2.1008, -1.9980, -1.9467],\n",
       "           [-1.0219, -0.9363, -0.8678,  ..., -2.0152, -2.0665, -2.0152],\n",
       "           [-1.0219, -0.9877, -0.9705,  ..., -1.9638, -2.0665, -2.1008]],\n",
       " \n",
       "          [[-0.5126, -0.4426, -0.4076,  ..., -1.0553, -1.0203, -0.9678],\n",
       "           [-0.4601, -0.5126, -0.4776,  ..., -0.9503, -1.0203, -1.0378],\n",
       "           [-0.3901, -0.4601, -0.4076,  ..., -0.9503, -1.0028, -0.9678],\n",
       "           ...,\n",
       "           [-0.6702, -0.6001, -0.5651,  ..., -1.6331, -1.5280, -1.4755],\n",
       "           [-0.6702, -0.6001, -0.4951,  ..., -1.5280, -1.5980, -1.5280],\n",
       "           [-0.6702, -0.6352, -0.6176,  ..., -1.4755, -1.5805, -1.6155]],\n",
       " \n",
       "          [[ 0.0082,  0.0605,  0.0953,  ..., -0.6541, -0.6018, -0.5321],\n",
       "           [ 0.0605,  0.0082,  0.0605,  ..., -0.5495, -0.6193, -0.6367],\n",
       "           [ 0.1302,  0.0953,  0.1476,  ..., -0.5844, -0.6193, -0.5670],\n",
       "           ...,\n",
       "           [-0.3404, -0.2707, -0.2358,  ..., -1.1596, -1.0550, -1.0027],\n",
       "           [-0.3055, -0.2358, -0.1487,  ..., -1.0550, -1.1073, -1.0550],\n",
       "           [-0.2532, -0.2184, -0.2184,  ..., -0.9678, -1.0724, -1.1073]]]]),\n",
       " tensor([[[[ 1.8037,  1.9064,  1.9749,  ...,  1.5810,  1.5468,  1.5125],\n",
       "           [ 1.9235,  1.9235,  1.8893,  ...,  1.5810,  1.5468,  1.5125],\n",
       "           [ 2.0263,  1.9578,  1.8379,  ...,  1.5810,  1.5468,  1.5125],\n",
       "           ...,\n",
       "           [ 0.3481,  0.2453,  0.1768,  ...,  1.7009,  1.7180,  1.7352],\n",
       "           [ 0.3138,  0.2111,  0.1768,  ...,  1.6838,  1.7180,  1.7523],\n",
       "           [ 0.2796,  0.1768,  0.1597,  ...,  1.6838,  1.7352,  1.7694]],\n",
       " \n",
       "          [[ 1.4132,  1.5182,  1.5707,  ...,  1.0280,  0.9930,  0.9580],\n",
       "           [ 1.5357,  1.5357,  1.4832,  ...,  1.0280,  0.9930,  0.9580],\n",
       "           [ 1.6408,  1.5532,  1.4307,  ...,  1.0280,  0.9930,  0.9580],\n",
       "           ...,\n",
       "           [-0.2150, -0.3375, -0.3901,  ...,  1.1331,  1.1506,  1.1681],\n",
       "           [-0.2500, -0.3550, -0.3901,  ...,  1.1155,  1.1506,  1.1856],\n",
       "           [-0.2850, -0.3901, -0.4076,  ...,  1.1155,  1.1681,  1.2031]],\n",
       " \n",
       "          [[ 0.9494,  1.0714,  1.1759,  ...,  0.3393,  0.3045,  0.2696],\n",
       "           [ 1.0714,  1.0888,  1.1062,  ...,  0.3393,  0.3045,  0.2696],\n",
       "           [ 1.1759,  1.1237,  1.0365,  ...,  0.3393,  0.3045,  0.2696],\n",
       "           ...,\n",
       "           [-0.6018, -0.7238, -0.7587,  ...,  0.4788,  0.4962,  0.5136],\n",
       "           [-0.6367, -0.7413, -0.7761,  ...,  0.4614,  0.4962,  0.5311],\n",
       "           [-0.6715, -0.7761, -0.7936,  ...,  0.4614,  0.5136,  0.5485]]]]),\n",
       " tensor([[[[ 0.6906,  0.6734,  0.7248,  ..., -0.1999,  0.5536,  0.0741],\n",
       "           [ 0.6221,  0.5707,  0.6221,  ...,  0.2282,  0.3994, -0.5424],\n",
       "           [ 0.5878,  0.5193,  0.5536,  ...,  0.4679, -0.0972, -0.9192],\n",
       "           ...,\n",
       "           [ 0.1939,  0.1597,  0.2282,  ..., -0.2171, -0.0972, -0.1143],\n",
       "           [ 0.2282,  0.2111,  0.2796,  ..., -0.3883, -0.3027, -0.2171],\n",
       "           [ 0.2624,  0.2282,  0.1426,  ..., -0.4739, -0.4739, -0.3198]],\n",
       " \n",
       "          [[-0.0924, -0.1275, -0.0574,  ..., -0.7927, -0.0224, -0.5126],\n",
       "           [-0.1450, -0.1800, -0.1275,  ..., -0.3200, -0.1450, -1.1253],\n",
       "           [-0.1800, -0.2325, -0.1975,  ..., -0.0574, -0.6176, -1.4755],\n",
       "           ...,\n",
       "           [-0.7402, -0.7927, -0.7227,  ..., -0.2500, -0.1099, -0.0749],\n",
       "           [-0.7052, -0.7577, -0.7052,  ..., -0.4076, -0.3200, -0.1975],\n",
       "           [-0.6702, -0.7052, -0.7927,  ..., -0.4601, -0.4601, -0.3375]],\n",
       " \n",
       "          [[-0.7761, -0.7936, -0.7413,  ..., -1.3513, -0.5844, -1.0550],\n",
       "           [-0.7936, -0.8807, -0.8284,  ..., -0.8807, -0.6890, -1.6302],\n",
       "           [-0.8458, -0.9330, -0.8981,  ..., -0.6018, -1.1596, -1.8044],\n",
       "           ...,\n",
       "           [-0.9853, -1.0027, -0.9156,  ..., -0.5321, -0.3927, -0.3578],\n",
       "           [-0.9678, -1.0027, -0.9504,  ..., -0.6715, -0.5844, -0.4450],\n",
       "           [-0.9504, -0.9853, -1.1073,  ..., -0.7413, -0.7413, -0.5495]]]])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed92193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28fc2439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 7])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "866d39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target[q].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "024acd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(534.3400, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb98c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    " for q in range(nq):\n",
    "        output = torch.zeros(model.meta['outputdim'], ni).cuda()\n",
    "        for imi in range(ni):\n",
    "\n",
    "            # compute output vector for image imi\n",
    "            output[:, imi] = model(input[q][imi].cuda()).squeeze()\n",
    "\n",
    "        # reducing memory consumption:\n",
    "        # compute loss for this query tuple only\n",
    "        # then, do backward pass for one tuple only\n",
    "        # each backward pass gradients will be accumulated\n",
    "        # the optimization step is performed for the full batch later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3543d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtarget\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379eb1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "049f4344-c456-48e9-a22e-cfe706968548",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs2qpool = torch.randperm(len(train_dataset.qpool))[:train_dataset.qsize]\n",
    "qidxs = [train_dataset.qpool[i] for i in idxs2qpool]\n",
    "pidxs = [train_dataset.ppool[i] for i in idxs2qpool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f37594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25a3234-aefd-456b-88d8-4ce7246cbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs2images = torch.randperm(len(train_dataset.images))[:train_dataset.poolsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e11a68a-4024-463a-875f-19e625b5a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Extracting descriptors for query images...\n",
      ">>>> 2000/2000 done...\n"
     ]
    }
   ],
   "source": [
    "# no gradients computed, to reduce memory and increase speed\n",
    "with torch.no_grad():\n",
    "\n",
    "    print('>> Extracting descriptors for query images...')\n",
    "    # prepare query loader\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ImagesFromList(root='', images=[train_dataset.images[i] for i in qidxs], imsize=train_dataset.imsize, transform=train_dataset.transform),\n",
    "        batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    qvecs = torch.zeros(384, len(qidxs)).cuda()\n",
    "    for i, input in enumerate(loader):\n",
    "        qvecs[:, i] = model(input.cuda())[0]\n",
    "        if (i+1) % print_freq == 0 or (i+1) == len(qidxs):\n",
    "            print('\\r>>>> {}/{} done...'.format(i+1, len(qidxs)), end='')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4946fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 20000/20000 done...\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  \n",
    "    # prepare negative pool data loader\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ImagesFromList(root='', images=[train_dataset.images[i] for i in idxs2images], imsize=train_dataset.imsize, transform=train_dataset.transform),\n",
    "        batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n",
    "    )\n",
    "    # extract negative pool vectors\n",
    "    poolvecs = torch.zeros(384, len(idxs2images)).cuda()\n",
    "    for i, input in enumerate(loader):\n",
    "        poolvecs[:, i] = model(input.cuda())[0]\n",
    "        if (i+1) % print_freq == 0 or (i+1) == len(idxs2images):\n",
    "            print('\\r>>>> {}/{} done...'.format(i+1, len(idxs2images)), end='')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67ebe983",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.mm(poolvecs.t(), qvecs)\n",
    "scores, ranks = torch.sort(scores, dim=0, descending=True)\n",
    "avg_ndist = torch.tensor(0).float().cuda()  # for statistics\n",
    "n_ndist = torch.tensor(0).float().cuda()  # f or statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c580b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ndist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ffa682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nidxs = []\n",
    "for q in range(len(qidxs)):\n",
    "    # do not use query cluster,\n",
    "    # those images are potentially positive\n",
    "    qcluster = train_dataset.cat_ids[qidxs[q]]\n",
    "    clusters = [qcluster]\n",
    "    nidxs = []\n",
    "    r = 0\n",
    "    while len(nidxs) < train_dataset.nnum:\n",
    "        potential = idxs2images[ranks[r, q]]\n",
    "        # take at most one image from the same cluster\n",
    "        if not train_dataset.cat_ids[potential] in clusters:\n",
    "            nidxs.append(potential)\n",
    "            clusters.append(train_dataset.cat_ids[potential])\n",
    "            avg_ndist += torch.pow(qvecs[:,q]-poolvecs[:,ranks[r, q]]+1e-6, 2).sum(dim=0).sqrt()\n",
    "            n_ndist += 1\n",
    "        r += 1\n",
    "    nidxs.append(nidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8288c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd27b41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(340402.8438, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_ndist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8cec7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.040283203125"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_ndist/n_ndist).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b74cfbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(56896),\n",
       " tensor(105003),\n",
       " tensor(13459),\n",
       " tensor(31224),\n",
       " tensor(6572),\n",
       " [...]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of negative examples\n",
    "self.nidxs = []\n",
    "for q in range(len(self.qidxs)):\n",
    "    # do not use query cluster,\n",
    "    # those images are potentially positive\n",
    "    qcluster = self.clusters[self.qidxs[q]]\n",
    "    clusters = [qcluster]\n",
    "    nidxs = []\n",
    "    r = 0\n",
    "    while len(nidxs) < self.nnum:\n",
    "        potential = idxs2images[ranks[r, q]]\n",
    "        # take at most one image from the same cluster\n",
    "        if not self.clusters[potential] in clusters:\n",
    "            nidxs.append(potential)\n",
    "            clusters.append(self.clusters[potential])\n",
    "            avg_ndist += torch.pow(qvecs[:,q]-poolvecs[:,ranks[r, q]]+1e-6, 2).sum(dim=0).sqrt()\n",
    "            n_ndist += 1\n",
    "        r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f9145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1ebf6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d203db1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46085374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37c18348-dc40-4077-b00e-12e99d42095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.genericdataset.ImagesFromList"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImagesFromList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a20cee56-c216-4fcf-91e5-ff8a0bf998a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108027"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "745cca53-64e1-49dd-b5d5-852efd1ec7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115539\n",
      "117070\n",
      "109538\n",
      "111261\n",
      "116886\n",
      "110338\n",
      "118118\n",
      "115210\n",
      "116564\n",
      "116949\n",
      "110661\n",
      "117656\n",
      "115383\n",
      "111437\n",
      "108097\n",
      "111752\n",
      "114793\n",
      "114639\n",
      "118384\n",
      "115741\n",
      "112308\n",
      "116638\n",
      "113378\n",
      "117951\n",
      "113495\n",
      "115626\n",
      "114773\n",
      "116429\n",
      "110721\n",
      "116517\n",
      "117240\n",
      "108173\n",
      "112018\n",
      "117508\n",
      "115137\n",
      "113322\n",
      "115967\n",
      "118426\n",
      "114462\n",
      "110965\n",
      "110888\n",
      "117424\n",
      "110559\n",
      "114893\n",
      "117374\n",
      "111179\n",
      "110355\n",
      "109650\n",
      "111155\n",
      "113171\n",
      "110071\n",
      "119317\n",
      "115738\n",
      "119133\n",
      "109937\n",
      "117473\n",
      "118889\n",
      "114714\n",
      "116158\n",
      "117537\n",
      "119104\n",
      "110349\n",
      "113608\n",
      "109161\n",
      "117838\n",
      "119478\n",
      "112228\n",
      "115009\n",
      "118157\n",
      "112793\n",
      "115401\n",
      "116930\n",
      "110644\n",
      "110313\n",
      "110673\n",
      "118003\n",
      "109964\n",
      "110123\n",
      "118478\n",
      "113951\n",
      "112446\n",
      "119675\n",
      "115562\n",
      "110978\n",
      "108204\n",
      "109249\n",
      "118843\n",
      "115415\n",
      "117926\n",
      "119244\n",
      "114554\n",
      "110223\n",
      "110804\n",
      "117228\n",
      "114126\n",
      "113506\n",
      "109881\n",
      "108926\n",
      "113690\n",
      "118636\n",
      "119594\n",
      "109074\n",
      "112008\n",
      "116905\n",
      "108613\n",
      "111007\n",
      "114478\n",
      "108484\n",
      "119349\n",
      "115678\n",
      "109918\n",
      "108387\n",
      "117387\n",
      "108088\n",
      "110851\n",
      "110102\n",
      "118261\n",
      "117673\n",
      "114426\n",
      "118465\n",
      "110625\n",
      "117289\n",
      "117705\n",
      "117200\n",
      "110431\n",
      "119277\n",
      "115953\n",
      "112738\n",
      "112459\n",
      "109217\n",
      "108868\n",
      "117147\n",
      "112240\n",
      "118783\n",
      "119068\n",
      "112835\n",
      "117164\n",
      "115103\n",
      "110228\n",
      "112741\n",
      "113335\n",
      "109329\n",
      "111813\n",
      "110113\n",
      "119880\n",
      "113148\n",
      "114556\n",
      "116386\n",
      "117607\n",
      "117171\n",
      "108187\n",
      "116956\n",
      "112455\n",
      "110566\n",
      "113853\n",
      "108227\n",
      "111834\n",
      "116691\n",
      "114770\n",
      "110403\n",
      "110456\n",
      "111840\n",
      "112753\n",
      "116329\n",
      "110738\n",
      "112473\n",
      "109310\n",
      "113550\n",
      "109856\n",
      "111513\n",
      "115513\n",
      "117610\n",
      "110047\n",
      "110924\n",
      "111478\n",
      "118331\n",
      "118790\n",
      "115536\n",
      "112646\n",
      "119158\n",
      "118271\n",
      "111600\n",
      "117420\n",
      "118829\n",
      "113918\n",
      "113669\n",
      "108229\n",
      "109458\n",
      "112109\n"
     ]
    }
   ],
   "source": [
    "for i in qidxs:\n",
    "    if i > len(train_dataset.images):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34979ac6-d01b-416c-bb80-76d5f4519bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "retrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
